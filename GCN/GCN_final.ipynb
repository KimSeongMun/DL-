{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdkit-pypi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -LO  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
    "\n",
    "!conda install -y -c rdkit rdkit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o ZINC.smiles https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning/master/Lec9/ZINC.smiles\n",
    "!curl -o vocab.npy https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning/master/Lec9/vocab.np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d95115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900fbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ZINC_smiles(file_name, num_mol):\n",
    "    f = open(file_name, 'r')\n",
    "    contents = f.readlines()\n",
    "\n",
    "    smi_list = [] # 분자의 그래프 input\n",
    "    logP_list = [] # 화학적 특성 \n",
    "\n",
    "    for i in tqdm_notebook(range(num_mol), desc='Reading Data'):\n",
    "        smi = contents[i].strip() # 파일에 데이터를 저장할때 공백부분을 제거해서 smi 저장\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        smi_list.append(smi)\n",
    "        logP_list.append(MolLogP(m))\n",
    "\n",
    "    logP_list = np.asarray(logP_list).astype(float)\n",
    "\n",
    "    return smi_list, logP_list\n",
    "\n",
    "\n",
    "def smiles_to_onehot(smi_list):\n",
    "    def smiles_to_vector(smiles, vocab, max_length):\n",
    "        while len(smiles) < max_length:\n",
    "            smiles += \" \"\n",
    "        vector = [vocab.index(str(x)) for x in smiles]\n",
    "        one_hot = np.zeros((len(vocab), max_length), dtype=int)\n",
    "        for i, elm in enumerate(vector):\n",
    "            one_hot[elm][i] = 1\n",
    "        return one_hot\n",
    "\n",
    "    vocab = np.load('./vocab.npy')\n",
    "    smi_total = []\n",
    "\n",
    "    for i, smi in tqdm_notebook(enumerate(smi_list), desc='Converting to One Hot'):\n",
    "        smi_onehot = smiles_to_vector(smi, list(vocab), 120)\n",
    "        smi_total.append(smi_onehot)\n",
    "\n",
    "    return np.asarray(smi_total)\n",
    "\n",
    "def convert_to_graph(smiles_list):\n",
    "    adj = []\n",
    "    adj_norm = []\n",
    "    features = []\n",
    "    maxNumAtoms = 50 # 최대 원자가 50개\n",
    "    for i in tqdm_notebook(smiles_list, desc='Converting to Graph'):\n",
    "        # Mol\n",
    "        iMol = Chem.MolFromSmiles(i.strip())\n",
    "        #Adj\n",
    "        iAdjTmp = Chem.rdmolops.GetAdjacencyMatrix(iMol)\n",
    "        # Feature\n",
    "        if( iAdjTmp.shape[0] <= maxNumAtoms):\n",
    "            # Feature-preprocessing\n",
    "            iFeature = np.zeros((maxNumAtoms, 58))\n",
    "            iFeatureTmp = []\n",
    "            for atom in iMol.GetAtoms():\n",
    "                iFeatureTmp.append( atom_feature(atom) ) ### atom features only\n",
    "            iFeature[0:len(iFeatureTmp), 0:58] = iFeatureTmp ### 0 padding for feature-set\n",
    "            features.append(iFeature)\n",
    "\n",
    "            # Adj-preprocessing\n",
    "            iAdj = np.zeros((maxNumAtoms, maxNumAtoms))\n",
    "            iAdj[0:len(iFeatureTmp), 0:len(iFeatureTmp)] = iAdjTmp + np.eye(len(iFeatureTmp))\n",
    "            adj.append(np.asarray(iAdj))\n",
    "    features = np.asarray(features)\n",
    "\n",
    "    return features, adj\n",
    "    \n",
    "def atom_feature(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                      ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "                                       'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "                                       'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "                                       'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()])    # (40, 6, 5, 6, 1)\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9739d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_smi, list_logP = read_ZINC_smiles('ZINC.smiles', 23040)\n",
    "list_feature, list_adj = convert_to_graph(list_smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ffa31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNDataset(Dataset):\n",
    "    def __init__(self, list_feature, list_adj, list_logP):\n",
    "        self.list_feature = list_feature\n",
    "        self.list_adj = list_adj\n",
    "        self.list_logP = list_logP\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_feature)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.list_feature[index], self.list_adj[index], self.list_logP[index]\n",
    "\n",
    "\n",
    "def partition(list_feature, list_adj, list_logP):\n",
    "    test_size = 0.2\n",
    "    val_size = 0.2\n",
    "    num_total = list_feature.shape[0]\n",
    "    num_train = int(num_total * (1 - test_size - val_size))\n",
    "    num_val = int(num_total * val_size)\n",
    "    num_test = int(num_total * test_size)\n",
    "\n",
    "    feature_train = list_feature[:num_train]\n",
    "    adj_train = list_adj[:num_train]\n",
    "    logP_train = list_logP[:num_train]\n",
    "    feature_val = list_feature[num_train:num_train + num_val]\n",
    "    adj_val = list_adj[num_train:num_train + num_val]\n",
    "    logP_val = list_logP[num_train:num_train + num_val]\n",
    "    feature_test = list_feature[num_total - num_test:]\n",
    "    adj_test = list_adj[num_total - num_test:]\n",
    "    logP_test = list_logP[num_total - num_test:]\n",
    "        \n",
    "    train_set = GCNDataset(feature_train, adj_train, logP_train)\n",
    "    val_set = GCNDataset(feature_val, adj_val, logP_val)\n",
    "    test_set = GCNDataset(feature_test, adj_test, logP_test)\n",
    "\n",
    "    partition = {\n",
    "        'train': train_set,\n",
    "        'val': val_set,\n",
    "        'test': test_set\n",
    "    }\n",
    "\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = partition(list_feature, list_adj, list_logP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, n_atom, act=None, bn=False):",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.use_bn = bn\n",
    "        self.linear = nn.Linear(in_dim, out_dim)",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.bn = nn.BatchNorm1d(n_atom)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x) #HW + b\n",
    "        out = torch.matmul(adj, out) #AHW + b\n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim,\n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadOut(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(ReadOut, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim= out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim, \n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sum(out, 1)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad28ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GCNNet, self).__init__()\n",
    "        out_dim2 = 128\n",
    "        hidden_dim = out_dim2\n",
    "        pred_dim1 = 128\n",
    "        pred_dim2 = 64\n",
    "        pred_dim3 = 32\n",
    "\n",
    "        self.gcn_layer1 = GCNLayer(in_dim = 58, out_dim = 116, n_atom = 50, bn=True)\n",
    "        self.gcn_layer2 = GCNLayer(in_dim = 116, out_dim = 232, n_atom = 50, bn=True)\n",
    "        self.gcn_layer3 = GCNLayer(in_dim = 232, out_dim = out_dim2, n_atom = 50, bn=True)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.readout = ReadOut(hidden_dim, \n",
    "                               pred_dim1,\n",
    "                               act=nn.ReLU())\n",
    "        self.pred1 = Predictor(pred_dim1,\n",
    "                               pred_dim2,\n",
    "                               act=nn.ReLU())\n",
    "        self.pred2 = Predictor(pred_dim2,\n",
    "                               pred_dim3,\n",
    "                               act=nn.ReLU())\n",
    "        self.pred3 = Predictor(pred_dim3,\n",
    "                               1)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        out, adj = self.gcn_layer1(x,adj)\n",
    "        out = self.relu(out)\n",
    "        out, adj = self.gcn_layer2(out, adj)\n",
    "        out = self.relu(out)\n",
    "        out, adj = self.gcn_layer3(out, adj)\n",
    "        out = self.relu(out)\n",
    "        out = self.readout(out)\n",
    "        out = self.pred1(out)\n",
    "        out = self.pred2(out)\n",
    "        out = self.pred3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, partition, optimizer, criterion):\n",
    "    train_batch_size = 256\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
    "                                              batch_size=train_batch_size, \n",
    "                                              shuffle=False, num_workers=2)\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get the inputs\n",
    "        list_feature, list_adj, list_logP = data\n",
    "\n",
    "        list_feature = list_feature.float()\n",
    "        list_adj = list_adj.float()\n",
    "        list_logP = list_logP.float().view(-1, 1)\n",
    "        outputs = net(list_feature, list_adj)\n",
    "\n",
    "        loss = criterion(outputs, list_logP)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, partition, criterion):\n",
    "    val_batch_size = 128\n",
    "    validloader = torch.utils.data.DataLoader(partition['val'], \n",
    "                                              batch_size=val_batch_size, \n",
    "                                              shuffle=False, num_workers=2)\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            list_feature, list_adj, list_logP = data\n",
    "\n",
    "            list_feature = list_feature.float()\n",
    "            list_adj = list_adj.float()\n",
    "            list_logP = list_logP.float().view(-1, 1)\n",
    "            outputs = net(list_feature, list_adj)\n",
    "\n",
    "            loss = criterion(outputs, list_logP)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(validloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06add182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, partition):\n",
    "    test_batch_size = 128\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
    "                                             batch_size=test_batch_size, \n",
    "                                             shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        logP_total = list()\n",
    "        pred_logP_total = list()\n",
    "        for data in testloader:\n",
    "            list_feature, list_adj, list_logP = data\n",
    "            list_feature = list_feature.float()\n",
    "            list_adj = list_adj.float()\n",
    "            list_logP = list_logP.float()\n",
    "            logP_total += list_logP.tolist()\n",
    "            list_logP = list_logP.view(-1, 1)\n",
    "            \n",
    "            outputs = net(list_feature, list_adj)\n",
    "            pred_logP_total += outputs.view(-1).tolist()\n",
    "\n",
    "        mae = mean_absolute_error(logP_total, pred_logP_total)\n",
    "        std = np.std(np.array(logP_total)-np.array(pred_logP_total))\n",
    "    \n",
    "    return logP_total, pred_logP_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97cc3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GCNNet()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay = 0.00001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29daa042",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 300\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(e):\n",
    "    ts = time.time()\n",
    "    train_loss = train(net=net, partition=partition, optimizer=optimizer, criterion=criterion)\n",
    "    val_loss = validate(net=net, partition=partition, criterion=criterion)\n",
    "    te = time.time()\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"Epoch : {}, Loss(train/val) : {:.4f}/{:.4f}, Time : {:.2f}\".format(epoch, train_loss, val_loss, te - ts))\n",
    "logP_total, pred_logP_total = test(net, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "logP_total = np.array(logP_total)\n",
    "pred_logP_total = np.array(pred_logP_total)\n",
    "model = LinearRegression()\n",
    "model.fit(logP_total.reshape(-1,1), pred_logP_total.reshape(-1,1))\n",
    "R2_score = model.score(logP_total.reshape(-1,1), pred_logP_total.reshape(-1,1))\n",
    "coefficient = float(model.coef_)\n",
    "intercept = float(model.intercept_)\n",
    "print(\" R2_score : {0} \\n Coefficient : {1} \\n Intercept : {2}\".format(R2_score, coefficient, intercept))\n",
    "X = np.linspace(min(logP_total),max(logP_total),100)\n",
    "Y = coefficient * X + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_epoch = list(range(e))\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.plot(list_epoch, train_losses, label = 'train_loss')\n",
    "ax1.plot(list_epoch, val_losses, label = 'val_loss')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "ax1.set_title('epoch vs loss')\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.scatter(logP_total, pred_logP_total, alpha = 0.2, s = 20, c = 'b')\n",
    "plt.plot(X, Y, 'c', linewidth=1 ,label=\" R2_score : {0} \\n Coefficient : {1} \\n Intercept : {2}\".format(R2_score, coefficient, intercept))\n",
    "ax2.set_xlabel('true')\n",
    "ax2.set_ylabel('predict')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "ax2.set_title('true vs predict')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
